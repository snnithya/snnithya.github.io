<!DOCTYPE html>
<html lang="en-us"><head>
    <meta charset="utf-8">
    <title>Nithya Shikarpur</title>

    <!-- mobile responsive meta -->
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Website of Nithya Shikarpur"> 
    <meta name="author" content="Nithya Shikarpur"> <meta name="generator" content="Hugo 0.84.0" /> 
    <!-- plugins --> 
    <link rel="stylesheet" href="https://snnithya.github.io/plugins/bootstrap/bootstrap.min.css "> 
    <link rel="stylesheet" href="https://snnithya.github.io/plugins/slick/slick.css "> 
    <link rel="stylesheet" href="https://snnithya.github.io/plugins/themify-icons/themify-icons.css "> 
    <link rel="stylesheet" href="https://snnithya.github.io/plugins/venobox/venobox.css ">  
    <!-- Main Stylesheet --> 
    <link rel="stylesheet" href="https://snnithya.github.io/scss/style.min.css" media="screen"> 
    <!--Favicon-->
    <link rel="shortcut icon" href="https://snnithya.github.io/images/favicon.png " type="image/x-icon">
    <link rel="icon" href="https://snnithya.github.io/images/favicon.png " type="image/x-icon"> 
    <!-- google analitycs -->
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
        ga('create', 'Your ID', 'auto');
        ga('send', 'pageview');
    </script>

</head><body>
<!-- preloader start -->
<div class="preloader">
  
</div>
<!-- preloader end -->
<!-- navigation -->
<header class="navigation">
  <div class="container">
    
    <nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0">
      <a class="navbar-brand mobile-view" href="https://snnithya.github.io/"><img class="img-fluid"
          src="https://snnithya.github.io/images/logo.png" alt="Nithya Shikarpur"></a>
      <button class="navbar-toggler border-0" type="button" data-toggle="collapse" data-target="#navigation">
        <i class="ti-menu"></i>
      </button>

      <div class="collapse navbar-collapse text-center" id="navigation">
        <div class="desktop-view">
          <ul class="navbar-nav mr-auto">
            
            <li class="nav-item">
              <a class="nav-link" href="https://www.youtube.com/channel/UCV4W-AUWGoYCenk24UJ5hEQ"><i class="ti-youtube"></i></a>
            </li>
            
            <li class="nav-item">
              <a class="nav-link" href="https://www.instagram.com/snnithya"><i class="ti-instagram"></i></a>
            </li>
            
            <li class="nav-item">
              <a class="nav-link" href="https://www.github.com/snnithya"><i class="ti-github"></i></a>
            </li>
            
            <li class="nav-item">
              <a class="nav-link" href="https://www.linkedin.com/in/snnithya/"><i class="ti-linkedin"></i></a>
            </li>
            
          </ul>
        </div>

        <a class="navbar-brand mx-auto desktop-view" href="https://snnithya.github.io/"><img class="img-fluid"
            src="https://snnithya.github.io/images/logo.png" alt="Nithya Shikarpur"></a>

        <ul class="navbar-nav">
          
          
          <li class="nav-item">
            <a class="nav-link" href="https://snnithya.github.io/portfolio">Portfolio</a>
          </li>
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="https://snnithya.github.io/resume/resume_nithya_shikarpur.pdf">Resume</a>
          </li>
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="https://snnithya.github.io/about">About</a>
          </li>
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="https://snnithya.github.io/contact">Contact</a>
          </li>
          
          
        </ul>

        

        
      </div>
    </nav>
  </div>
</header>
<!-- /navigation -->

<section class="section-sm">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 mx-auto">
                
                <a href="/categories/music-research" class="text-primary">Music research</a> 
                <h2>Interactive Music Generation Experiments</h2>
                <div class="mb-3 post-meta">
                    <span>By Nithya Shikarpur</span> 
                    <span class="border-bottom border-primary px-2 mx-1"></span>
                    <span>25 November 2023</span> 
                </div>
                
                <div class="content mb-5">
                    <h1 id="research-abstract">Research Abstract</h1>
<p>In Hindustani music, a tradition of North Indian Classical music, the melody is loosely constrained by the chosen composition but otherwise largely improvised in accordance with the raga grammar which are often realized at different levels of abstraction and conformity [<a href="https://www.researchgate.net/publication/321514698_Hindustani_Music_in_the_20th_Century">1</a>, <a href="https://www.ee.iitb.ac.in/student/~daplab/people/thesis/KKG_Thesis.pdf">2</a>]. Through this project I wish to develop a human-computer interactive system that enables artists and performers to explore novel creative spaces inspired by the Hindustani idiom. Through this study, I wish to focus on three aspects: music generation, controllability and interaction each with its own motivation as I will specify below.</p>
<p><strong>Generation</strong>: Previous works on generative modeling for Hindustani music [<a href="http://www.cs.cmu.edu/~dipanjan/pubs/frsm_gen.pdf">3</a>, <a href="https://computationalmusic.com/index.php">4</a>, <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10126388">5</a>, <a href="https://doi.org/10.1145/3493700.3493762">6</a>] approach the problem of generating raga music as midi-like notation. Hindustani music is a largely oral tradition, as a result, reducing the music to a midi-like notation gives up a lot of important information such as note ornamentations, dynamics, pitching etc. Hence, I propose 2 other methods to generate audio of this form: pitch contour based generation and audio waveform based generation. Pitch contour based generation [<a href="https://openreview.net/forum?id=UseMOjWENv">7</a>, <a href="https://ieeexplore.ieee.org/document/8341752">8</a>] can capture intricate melodic movements better than midi due to a finer resolution on the pitch axis; additionally DDSP [<a href="https://openreview.net/forum?id=B1x1ma4tDr">9</a>] can be used to synthesize pitch into realistic sounds. Audio waveform, on the other hand, captures much more detail (timbre, dynamics) than just pitch and is simultaneously more noisy due to the high sample rate, thus making it harder to model [<a href="https://arxiv.org/abs/1806.10474">10</a>]. I propose to encode the waveform using encoders [<a href="https://arxiv.org/pdf/2111.05011.pdf">11</a>, <a href="https://doi.org/10.48550/arXiv.2107.03312">12</a>,<a href="https://arxiv.org/abs/2210.13438">13</a>] and modeling this encoded sequence. Given their ability to model musical sequences effectively, I use transformers [<a href="https://arxiv.org/abs/1706.03762">14</a>,<a href="https://github.com/caillonantoine/msprior">15</a>,<a href="https://arxiv.org/abs/2302.13542">16</a>] with the objective of sequence continuation, i.e. given an input sequence, the model should construct a continuation that is likely in the given data distribution. The performance can be measured objectively by entropy in addition to hand-crafted metrics typical to Hindustani music such as the adherence to the notes of a raga (melodic mode), the pitch range of input output etc.</p>
<p><strong>Controllability</strong>: The idiom of Hindustani music presents a framework or fixed boundary within which improvisation can take place. For instance, a performer always performs with respect to a tonic frequency. By extension, this implies that generations have to be conditioned on a tonic frequency as well to be useful for interaction. Additionally, improvisation usually takes place within a raga framework which would be another conditioning signal for the generation. Apart from these global controls, I also propose to introduce certain aesthetic local controls to control the melodic trajectory of the generations such as controlling the amount of oscillations (gamaka)  in the generated melody, the average direction of pitch in the generations (up or down), and the average dynamics of the generation. Introducing controls to generation paves the possibility of a more creatively satisfying experience with the performer left feeling in control when they want to. Based on prior work on controllability [<a href="https://arxiv.org/abs/2302.13542">16</a>], I plan to evaluate this aspect based on correlation metrics between the input attributes and the corresponding extracted attributes from the generated signal.</p>
<p><strong>Interaction</strong>: Since this project aims to address the paradigm of human-computer interactive generation, it becomes essential to think about the user experience with this system [<a href="https://arxiv.org/abs/2010.05388">17</a>]. Creativity Support Index [<a href="https://doi.org/10.1145/2617588">18</a>,<a href="https://doi.org/10.1145/3313831.3376739">19</a>,<a href="https://nmh.no/en/research/publications/mixed-initiative-music-making">20</a>] is used to measure the ability of a tool to assist a user in creative work. To this effect, it systematically measures six factors including exploration, expressiveness, immersion, enjoyment, results worth effort, and collaboration through user surveys. With the goal of enabling creative exploration, I plan to focus more carefully on aspects of exploration, immersion and enjoyment while also keeping the other factors in mind when relevant. Additionally, interactions with professionals in the field will ensure that this work is relevant to the community of musicians who practice this style of music.</p>
<h1 id="interactive-demos">Interactive demos</h1>
<p>This is a PoC demo with interactive waveform generation. In this experiment, I use a modified version of <a href="https://arxiv.org/pdf/2111.05011.pdf">Rave</a> (continuous VAE) and <a href="https://github.com/caillonantoine/msprior">msprior</a> together. Rave is used to encode the waveform into more compact representation with a lower sampling rate. These tokens are then fed into msprior, a transformer decoder model that predicts these tokens autoregressively in real time. The data is from the Hindustani Raga Recognition dataset which consists of 116 hours of data, with 30 different ragas and over 55 vocal artists. The left and right audio channels is my voice input and the left audio channel is the model responding to my voice. This is a work in progress.</p>
<p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/LxsjODGGuRU" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

Artwork credit: <a href="https://craiyon.com">craiyon.com</a></p>
<h3 id="references">References</h3>
<p>[1] W. van der Meer. Hindustani Music in the 20th Century. Martinus Nijhoff Publishers, 1980.<br>
[2]  Ganguli, K. K., “A corpus based approach to the computational modeling of melody in raga music.,” PhD diss., Indian Institute of Technology, Bombay, 2019. <a href="https://www.ee.iitb.ac.in/student/~daplab/people/thesis/KKG_Thesis.pdf">https://www.ee.iitb.ac.in/student/~daplab/people/thesis/KKG_Thesis.pdf</a><br>
[3] Das D., Chowdhury M. FINITE STATE MODELS FOR GENERATION OF
HINDUSTANI CLASSICAL MUSIC. <a href="http://www.cs.cmu.edu/~dipanjan/pubs/frsm_gen.pdf">http://www.cs.cmu.edu/~dipanjan/pubs/frsm_gen.pdf</a><br>
[4] Vidwans, V. Computational music. <a href="https://computationalmusic.com/index.php">https://computationalmusic.com/index.php</a><br>
[5] Automatic Music Generation of Indian Classical Music based on Raga. (2023, April 7). IEEE Conference Publication | IEEE Xplore. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10126388">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10126388</a> <br>
[6] Viramgami, G., Gandhi, H., Naik, H., Mahajan, N., Venkatesh, P., Sahni, S., &amp; Singh, M. (2022). Indian Classical Music Synthesis. 5th Joint International Conference on Data Science &amp; Management of Data (9th ACM IKDD CODS and 27th COMAD). <a href="https://doi.org/10.1145/3493700.3493762">https://doi.org/10.1145/3493700.3493762</a> <br>
[7] Wu, Y. et. al. MIDI-DDSP: Detailed control of musical performance via hierarchical modeling. International Conference on Learning Representations, 2022. <a href="https://openreview.net/forum?id=UseMOjWENv">https://openreview.net/forum?id=UseMOjWENv</a><br>
[8] Xin Wang, Shinji Takaki, and Junichi Yamagishi. Autoregressive neural f0 model for statistical parametric speech synthesis. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 26(8):1406–1419. <a href="https://ieeexplore.ieee.org/document/8341752">https://ieeexplore.ieee.org/document/8341752</a>.<br>
[9] Engel, J. et. al. DDSP: Differentiable Digital Signal Processing. International Conference on Learning Representations, 2019. <a href="https://openreview.net/forum?id=B1x1ma4tDr">https://openreview.net/forum?id=B1x1ma4tDr</a><br>
[10] Dieleman, Sander, et al. The Challenge of Realistic Music Generation: Modelling Raw Audio at Scale. 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada. <a href="https://arxiv.org/abs/1806.10474">https://arxiv.org/abs/1806.10474</a>.<br>
[11] Caillon, A., &amp; Esling, P. (n.d.). RAVE: A variational autoencoder for fast and high-quality neural audio synthesis. <a href="https://arxiv.org/pdf/2111.05011.pdf">https://arxiv.org/pdf/2111.05011.pdf</a><br>
[12] Zeghidour, Neil, et al. “SoundStream: An End-To-End Neural Audio Codec.” Arxiv.org, 7 July 2021, arxiv.org/abs/2107.03312, <a href="https://doi.org/10.48550/arXiv.2107.03312">https://doi.org/10.48550/arXiv.2107.03312</a>. Accessed 16 Oct. 2022.<br>
[13] Défossez, Alexandre, et al. High Fidelity Neural Audio Compression. 24 Oct 2022. <a href="https://arxiv.org/abs/2210.13438">https://arxiv.org/abs/2210.13438</a>.<br>
[14] Vaswani, Ashish, et al. “Attention Is All You Need.” ArXiv.org, 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA, 2017, <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.<br>
[15] Caillon, A. (2023, September 14). MSPrior. GitHub. <a href="https://github.com/caillonantoine/msprior">https://github.com/caillonantoine/msprior</a><br>
[16] Devis, N., Demerlé, N., Nabi, S., Genova, D., &amp; Esling, P. (2023, February 27). Continuous descriptor-based control for deep audio synthesis. <a href="https://arxiv.org/abs/2302.13542">https://arxiv.org/abs/2302.13542</a><br>
[17] Huang, Cheng-Zhi, et al. AI SONG CONTEST: HUMAN-AI CO-CREATION in SONGWRITING. <a href="https://arxiv.org/abs/2010.05388">https://arxiv.org/abs/2010.05388</a><br>
[18] Cherry, E. C., &amp; Latulipe, C. (2014). Quantifying the Creativity Support of Digital Tools through the Creativity Support Index. ACM Transactions on Computer-Human Interaction, 21(4), 1–25. <a href="https://doi.org/10.1145/2617588">https://doi.org/10.1145/2617588</a><br>
[19] Ryan Louie, Andy Coenen, Cheng Zhi Huang, Michael Terry, and Carrie J. Cai. 2020. Novice-AI Music Co-Creation via AI-Steering Tools for Deep Generative Models. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI &lsquo;20). Association for Computing Machinery, New York, NY, USA, 1–13. <a href="https://doi.org/10.1145/3313831.3376739">https://doi.org/10.1145/3313831.3376739</a><br>
[20] J W Thelle, Notto. “Mixed-Initiative Music Making.”. PhD Diss. 2022 <a href="https://nmh.no/en/research/publications/mixed-initiative-music-making">https://nmh.no/en/research/publications/mixed-initiative-music-making</a>\</p>

                </div>

                
                
            </div>
        </div>
    </div>
</section>





<script>
    var indexURL = {
        {
            "index.json" | absURL
        }
    }
</script>

<!-- JS Plugins --> 

<script src="https://snnithya.github.io/plugins/jQuery/jquery.min.js"></script>

<script src="https://snnithya.github.io/plugins/bootstrap/bootstrap.min.js"></script>

<script src="https://snnithya.github.io/plugins/slick/slick.min.js"></script>

<script src="https://snnithya.github.io/plugins/venobox/venobox.min.js"></script>

<script src="https://snnithya.github.io/plugins/search/fuse.min.js"></script>

<script src="https://snnithya.github.io/plugins/search/mark.js"></script>

<script src="https://snnithya.github.io/plugins/search/search.js"></script>
 
<!-- Main Script --> 

<script src="https://snnithya.github.io/js/script.min.js"></script>



</body>

</html>