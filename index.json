[{"categories":["Music Research"],"contents":"Research Abstract In Hindustani music, a tradition of North Indian Classical music, the melody is loosely constrained by the chosen composition but otherwise largely improvised in accordance with the raga grammar which are often realized at different levels of abstraction and conformity [1, 2]. Through this project I wish to develop a human-computer interactive system that enables artists and performers to explore novel creative spaces inspired by the Hindustani idiom. Through this study, I wish to focus on three aspects: music generation, controllability and interaction each with its own motivation as I will specify below.\nGeneration: Previous works on generative modeling for Hindustani music [3, 4, 5, 6] approach the problem of generating raga music as midi-like notation. Hindustani music is a largely oral tradition, as a result, reducing the music to a midi-like notation gives up a lot of important information such as note ornamentations, dynamics, pitching etc. Hence, I propose 2 other methods to generate audio of this form: pitch contour based generation and audio waveform based generation. Pitch contour based generation [7, 8] can capture intricate melodic movements better than midi due to a finer resolution on the pitch axis; additionally DDSP [9] can be used to synthesize pitch into realistic sounds. Audio waveform, on the other hand, captures much more detail (timbre, dynamics) than just pitch and is simultaneously more noisy due to the high sample rate, thus making it harder to model [10]. I propose to encode the waveform using encoders [11, 12,13] and modeling this encoded sequence. Given their ability to model musical sequences effectively, I use transformers [14,15,16] with the objective of sequence continuation, i.e. given an input sequence, the model should construct a continuation that is likely in the given data distribution. The performance can be measured objectively by entropy in addition to hand-crafted metrics typical to Hindustani music such as the adherence to the notes of a raga (melodic mode), the pitch range of input output etc.\nControllability: The idiom of Hindustani music presents a framework or fixed boundary within which improvisation can take place. For instance, a performer always performs with respect to a tonic frequency. By extension, this implies that generations have to be conditioned on a tonic frequency as well to be useful for interaction. Additionally, improvisation usually takes place within a raga framework which would be another conditioning signal for the generation. Apart from these global controls, I also propose to introduce certain aesthetic local controls to control the melodic trajectory of the generations such as controlling the amount of oscillations (gamaka) in the generated melody, the average direction of pitch in the generations (up or down), and the average dynamics of the generation. Introducing controls to generation paves the possibility of a more creatively satisfying experience with the performer left feeling in control when they want to. Based on prior work on controllability [16], I plan to evaluate this aspect based on correlation metrics between the input attributes and the corresponding extracted attributes from the generated signal.\nInteraction: Since this project aims to address the paradigm of human-computer interactive generation, it becomes essential to think about the user experience with this system [17]. Creativity Support Index [18,19,20] is used to measure the ability of a tool to assist a user in creative work. To this effect, it systematically measures six factors including exploration, expressiveness, immersion, enjoyment, results worth effort, and collaboration through user surveys. With the goal of enabling creative exploration, I plan to focus more carefully on aspects of exploration, immersion and enjoyment while also keeping the other factors in mind when relevant. Additionally, interactions with professionals in the field will ensure that this work is relevant to the community of musicians who practice this style of music.\nInteractive demos This is a PoC demo with interactive waveform generation. In this experiment, I use a modified version of Rave (continuous VAE) and msprior together. Rave is used to encode the waveform into more compact representation with a lower sampling rate. These tokens are then fed into msprior, a transformer decoder model that predicts these tokens autoregressively in real time. The data is from the Hindustani Raga Recognition dataset which consists of 116 hours of data, with 30 different ragas and over 55 vocal artists. The left and right audio channels is my voice input and the left audio channel is the model responding to my voice. This is a work in progress.\n   Artwork credit: craiyon.com\nReferences [1] W. van der Meer. Hindustani Music in the 20th Century. Martinus Nijhoff Publishers, 1980.\n[2] Ganguli, K. K., “A corpus based approach to the computational modeling of melody in raga music.,” PhD diss., Indian Institute of Technology, Bombay, 2019. https://www.ee.iitb.ac.in/student/~daplab/people/thesis/KKG_Thesis.pdf\n[3] Das D., Chowdhury M. FINITE STATE MODELS FOR GENERATION OF HINDUSTANI CLASSICAL MUSIC. http://www.cs.cmu.edu/~dipanjan/pubs/frsm_gen.pdf\n[4] Vidwans, V. Computational music. https://computationalmusic.com/index.php\n[5] Automatic Music Generation of Indian Classical Music based on Raga. (2023, April 7). IEEE Conference Publication | IEEE Xplore. https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\u0026amp;arnumber=10126388 [6] Viramgami, G., Gandhi, H., Naik, H., Mahajan, N., Venkatesh, P., Sahni, S., \u0026amp; Singh, M. (2022). Indian Classical Music Synthesis. 5th Joint International Conference on Data Science \u0026amp; Management of Data (9th ACM IKDD CODS and 27th COMAD). https://doi.org/10.1145/3493700.3493762 [7] Wu, Y. et. al. MIDI-DDSP: Detailed control of musical performance via hierarchical modeling. International Conference on Learning Representations, 2022. https://openreview.net/forum?id=UseMOjWENv\n[8] Xin Wang, Shinji Takaki, and Junichi Yamagishi. Autoregressive neural f0 model for statistical parametric speech synthesis. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 26(8):1406–1419. https://ieeexplore.ieee.org/document/8341752.\n[9] Engel, J. et. al. DDSP: Differentiable Digital Signal Processing. International Conference on Learning Representations, 2019. https://openreview.net/forum?id=B1x1ma4tDr\n[10] Dieleman, Sander, et al. The Challenge of Realistic Music Generation: Modelling Raw Audio at Scale. 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada. https://arxiv.org/abs/1806.10474.\n[11] Caillon, A., \u0026amp; Esling, P. (n.d.). RAVE: A variational autoencoder for fast and high-quality neural audio synthesis. https://arxiv.org/pdf/2111.05011.pdf\n[12] Zeghidour, Neil, et al. “SoundStream: An End-To-End Neural Audio Codec.” Arxiv.org, 7 July 2021, arxiv.org/abs/2107.03312, https://doi.org/10.48550/arXiv.2107.03312. Accessed 16 Oct. 2022.\n[13] Défossez, Alexandre, et al. High Fidelity Neural Audio Compression. 24 Oct 2022. https://arxiv.org/abs/2210.13438.\n[14] Vaswani, Ashish, et al. “Attention Is All You Need.” ArXiv.org, 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA, 2017, https://arxiv.org/abs/1706.03762.\n[15] Caillon, A. (2023, September 14). MSPrior. GitHub. https://github.com/caillonantoine/msprior\n[16] Devis, N., Demerlé, N., Nabi, S., Genova, D., \u0026amp; Esling, P. (2023, February 27). Continuous descriptor-based control for deep audio synthesis. https://arxiv.org/abs/2302.13542\n[17] Huang, Cheng-Zhi, et al. AI SONG CONTEST: HUMAN-AI CO-CREATION in SONGWRITING. https://arxiv.org/abs/2010.05388\n[18] Cherry, E. C., \u0026amp; Latulipe, C. (2014). Quantifying the Creativity Support of Digital Tools through the Creativity Support Index. ACM Transactions on Computer-Human Interaction, 21(4), 1–25. https://doi.org/10.1145/2617588\n[19] Ryan Louie, Andy Coenen, Cheng Zhi Huang, Michael Terry, and Carrie J. Cai. 2020. Novice-AI Music Co-Creation via AI-Steering Tools for Deep Generative Models. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI \u0026lsquo;20). Association for Computing Machinery, New York, NY, USA, 1–13. https://doi.org/10.1145/3313831.3376739\n[20] J W Thelle, Notto. “Mixed-Initiative Music Making.”. PhD Diss. 2022 https://nmh.no/en/research/publications/mixed-initiative-music-making\\\n","permalink":"https://snnithya.github.io/portfolio/interactive-generation/","tags":["Indian Music","Generative Modeling","HCI"],"title":"Interactive Music Generation Experiments"},{"categories":["Music Research"],"contents":"For my HCI course project in Fall 2022, I decided to study the role of visual aids such as piano roll and music notation in aiding the memorization and reproduction of note onset timings in a jazz improvisation with respect to a backing track. The motivation for this kind of study is two-fold. Firstly transcription and imitation are essential while learning jazz improvisation. The task we present in our study is a form of rhythmic transcription and imitation which would be very beneficial for students of jazz. Secondly, previous studies, such as Hilebrandt et. al. have shown that for temporal information (such as the position of note onsets) is better captured in the audio modality and in the case of multimodal input (audio-visual), the visual modality doesn\u0026rsquo;t seem to play an important role at all. However, my hypothesis was that in the context of music, visual inputs must have some significant role to play in memorization/understanding of temporal information given the popularity of visualizations such as score notation among western musician performers, piano rolls among digital audio workstations etc.\nFor this project, I developed an interface. Users had to listen to (and in some cases, see) the snippets of improvisation, memorize the position of the notes with respect to the bass and drums, and tap out the note positions first along with the improvisation playing in the background, then without the improvisation (i.e. only with the bass and drums). Through quantitative and qualitative analysis, I show that visualizations do indeed help with a more accurate memorization of the note onsets.\nFor more detail, please read my report. This was preliminary work and I would love to hear feedback if you have any!\n","permalink":"https://snnithya.github.io/portfolio/rhythm_visualization/","tags":["HCI","Jazz","Publication and Patent"],"title":"Tap Tap Revolution: A study of rhythm reproduction from jazz solos with visual aids"},{"categories":["Music Research"],"contents":"Update: This paper won the \u0026lsquo;best special call paper award\u0026rsquo; at ISMIR 2022.\nIn this study, we try to use audio information (pitch and voicing) along with video information (x, y coordinated of the right and left wrists of a singer) to determine the raga of 12 s clips. We were able to show that video data, if incorporated correctly, can help correct the mistakes made by a classifier using solely audio data. We show quantitative results backed by qualitative commentary.\nLinks  Paper Supplementary Material  ","permalink":"https://snnithya.github.io/portfolio/gesture_analysis/","tags":["Indian Music","Publication and Patent"],"title":"Raga classification from vocal performance using multimodal analysis"},{"categories":["Music"],"contents":"Videos from my performance at Nexgen Multiarts festival organized by Kabir Centre For Arts \u0026amp; Culture, Montreal, Canada on 21 Oct 2023.\nArtists\n Harmonium: Sri Ninad Puranik Tabla: Sri Chinmay Kulkarni Vocals: Nithya Shikarpur  Raga Gaud Sarang   Jagadhodharana | Purandaradasa Devaranama   ","permalink":"https://snnithya.github.io/portfolio/kabir-program/","tags":["Indian Music","Performance"],"title":"Music Performance in Montreal"},{"categories":["Music"],"contents":"This is an adaptation of the famous Bollywood song Piyu Bole, from the movie Parineeta with a twist. Half way through the cover, we jazzed things up a bit :) I hope you enjoy it!\nCredits:\n Singing, Animations and vocal arrangements - Nithya Shikarpur Flute fills - Pranav Shikarpur All other instruments, mixing - Shikhar Rastogi    ","permalink":"https://snnithya.github.io/portfolio/piyu_bole/","tags":["Arrangement","Fusion","Indian Music","Jazz","Animation","Art"],"title":"Piyu Bole - with a jazz twist"},{"categories":["Music Research"],"contents":"This work was done with Asawari Keskar and Dr. Preeti Rao at the Digital Audio Processing Lab, IIT Bombay.\nThis is a study of a special type of concert in Hindustani classical music called Jasrangi Jugalbandi. This type of concert involves a male and a female singer singing the same keyboard notes in 2 different ragas (scales) using the concept of mode shifting or murchana.\nThis projected resulted in a paper which was presented in ISMIR 2021 and can be found here.\nBackground Transposition In various forms of modal music, it is common to obtain multiple scales from the same set of notes by just changing the tonic note. This concept is called murchana in Indian classical music. Below is an example of such a type of murchana where the singer, Gayatri, sings the notes of raga Chandrakauns in the concert tonic, i.e. G# (playing in the tanpuras), followed by the same keyboard notes in raga Madhukauns by assuming the tonic to be the madhyam (fourth note), i.e. C# above the concert tonic.   The video is a snippet from a Ranjani Gayatri concert posted on YouTube available here.\nJasrangi Jugalbandi (JJ) This type of concert was first conceptualised by Pt. Jasraj in the year 2012. Since male and female vocalists usually have natural vocal ranges that differ by around 5-7 semitones, this concert allows them to sing in their comfortable tonics (Sa) using the concept of murchana with the same notes.\nHere is an example of such a concert by Dr. Ashwini Bhide Deshpande and Pt. Sanjeev Abhyankar singing ragas Abhogi and Kalavati respectively.   Our Research Motivations Through our conversations with musicians we identified the following challenges during interaction of singers in this format of concert:\n The singers have to preserve raga specific characteristics They also have to meaningfully link phrases  Hence we sought to analyse the following:\n Study the extent to which individual raga-specific characteristics are preserved in a JJ concert Study the interaction between the 2 singers in the JJ concert  Research Material Here is a video of me talking about our research paper:   Here is our poster about the paper:    #the-canvas { border: 1px solid black; direction: ltr; width: 100%; height: auto; display: none; } #paginator { display: none; text-align: center; margin-bottom: 10px; } #loadingWrapper { display: none; justify-content: center; align-items: center; width: 100%; height: 350px; } #loading { display: inline-block; width: 50px; height: 50px; border: 3px solid #d2d0d0; ; border-radius: 50%; border-top-color: #383838; animation: spin 1s ease-in-out infinite; -webkit-animation: spin 1s ease-in-out infinite; } @keyframes spin { to { -webkit-transform: rotate(360deg); } } @-webkit-keyframes spin { to { -webkit-transform: rotate(360deg); } }  Previous Next \u0026nbsp; \u0026nbsp; Page:  /       window.onload = function() { var url = \"https:\\/\\/snnithya.github.io\\/\" + 'portfolio\\/01_jj\\/Poster 2.0.pdf'; var hidePaginator = \"\" === \"true\"; var hideLoader = \"\" === \"true\"; var selectedPageNum = parseInt(\"\") || 1; var pdfjsLib = window['pdfjs-dist/build/pdf']; pdfjsLib.GlobalWorkerOptions.workerSrc = \"https:\\/\\/snnithya.github.io\\/\" + '/js/pdf-js/build/pdf.worker.js'; var pdfDoc = null, pageNum = selectedPageNum, pageRendering = false, pageNumPending = null, scale = 3, canvas = document.getElementById('the-canvas'), ctx = canvas.getContext('2d'), paginator = document.getElementById(\"paginator\"), loadingWrapper = document.getElementById('loadingWrapper'); showPaginator(); showLoader(); function renderPage(num) { pageRendering = true; pdfDoc.getPage(num).then(function(page) { var viewport = page.getViewport({ scale: scale }); canvas.height = viewport.height; canvas.width = viewport.width; var renderContext = { canvasContext: ctx, viewport: viewport }; var renderTask = page.render(renderContext); renderTask.promise.then(function() { pageRendering = false; showContent(); if (pageNumPending !== null) { renderPage(pageNumPending); pageNumPending = null; } }); }); document.getElementById('page_num').textContent = num; } function showContent() { loadingWrapper.style.display = 'none'; canvas.style.display = 'block'; } function showLoader() { if (hideLoader) return loadingWrapper.style.display = 'flex'; canvas.style.display = 'none'; } function showPaginator() { if (hidePaginator) return paginator.style.display = 'block'; } function queueRenderPage(num) { if (pageRendering) { pageNumPending = num; } else { renderPage(num); } } function onPrevPage() { if (pageNum = pdfDoc.numPages) { return; } pageNum++; queueRenderPage(pageNum); } document.getElementById('next').addEventListener('click', onNextPage); pdfjsLib.getDocument(url).promise.then(function(pdfDoc_) { pdfDoc = pdfDoc_; var numPages = pdfDoc.numPages; document.getElementById('page_count').textContent = numPages; if (pageNum  numPages) { pageNum = numPages } renderPage(pageNum); }); } \n","permalink":"https://snnithya.github.io/portfolio/jj/","tags":["Indian Music","Publication and Patent"],"title":"Computational Analysis of Melodic Mode Switching"},{"categories":["Music"],"contents":"Videos from my performance at a house concert in Framingham, MA on 17th December, 2022.\nArtists\n Harmonium: Sri Hirak Modi Tabla: Sri Rajesh Pai Vocals: Nithya Shikarpur  Raga Bhimpalas      Raga Sohani   ","permalink":"https://snnithya.github.io/portfolio/boston-program/","tags":["Indian Music","Performance"],"title":"Music Performance in MA, USA"},{"categories":["Music"],"contents":"Videos from my performance at Indian Music Experience, Bangalore on 17th December, 2022.\nArtists\n Harmonium: Sri Rohit Bharadwaj Tabla: Sri Anirudh Shankar Vocals: Nithya Shikarpur  Raga Chayanat, Madhyalay and Tarana   Diwana Kiye Shyam, Dadra   ","permalink":"https://snnithya.github.io/portfolio/17_dec_ime_program/","tags":["Indian Music","Performance"],"title":"Music Program at IME, 17th December 2022"},{"categories":["Music Research"],"contents":"I came across Max/MSP/Jitter about a month ago with the course called Twisted Signals by Prof. Jesse Stiles. Inspired by the flexibility of the language, I decided to program a movement based music synthesis program. As a dancer, I wanted to explore the possibility of letting the music follow my lead instead of choreographing my steps to the music. This is a demo of the project in its very initial stages.\nHow it works  I capture movement from my camera at 500 fps. Each frame is captured as a matrix. I subtract a 30 ms delayed frame matrix from the current frame matrix in order to capture the movement on an individual. The difference of frame matrices are used to decide the midi note value and velocity. Note values increases from the left to the right of the screen and the velocity is determined by how \u0026ldquo;big\u0026rdquo; the movement is.  All sounds from this demo are produced with Spitfire Audio\u0026rsquo;s plugins.\n Screenshot of max patch being used\n  Demo of Project   Possible directions to work in  Try to detect the skeleton of a person (possibly using OpenPose) and use that data to control the music. This would make the sound more controllable. Interact with dancers of different styles and understand their perspective and thoughts. Think about how this idea can be used to visualise music or hear dance. This could help people with hearing or seeing disabilities. I want to also experiment with Indian sounds and Indian forms to see how that works.  I would love to know any thoughts or feedback on this project. Please feel free to shoot me a mail or get in touch on social media :)\n","permalink":"https://snnithya.github.io/portfolio/max_demo/","tags":["Art"],"title":"Movement Led Music Synthesis Program - Work In Progress"},{"categories":["Music"],"contents":"This was a fun cover of the song labb par aaye, from the tv series Bandish Bandits, that I did with my friend Anjali Upadhyaya. Throughout our schooling, both of us were very interested in the Indian Arts. We finally got around to making something together :) Hope you like it!\nCredits:\n Anjali Upadhyaya - Dancing, bol recitation Nithya Shikarpur - Singing, and all other song related aspects    ","permalink":"https://snnithya.github.io/portfolio/labb_par_aaye/","tags":["Indian Music","Arrangement"],"title":"Labb Par Aaye - Song + Dance"},{"categories":["Music"],"contents":"I gave vocals for the song Geidi Prima on Trey Kams' album Supermassive!\n  ","permalink":"https://snnithya.github.io/portfolio/geidi_prima/","tags":["Indian Music","Performance"],"title":"Geidi Prima"},{"categories":["Music"],"contents":"This is my arrangement of the song Phir Le Aaya Dil from the movie Barfi. This is an a capella arrangement and only involves my voice. This was sung, recorded and put together by me at home during the lockdown.\n  ","permalink":"https://snnithya.github.io/portfolio/plad/","tags":["Arrangement","Indian Music","Animation","Quarantine Absurdity"],"title":"Phir Le Aaya Dil (a cappella)"},{"categories":["Music"],"contents":"This is my arrangement of fly me to the moon by Frank Sinatra. I decided to make it a little Indian by adding some swaras in the second half of the song. This was sung, recorded and put together by me at home during the lockdown.\n  ","permalink":"https://snnithya.github.io/portfolio/fmttm/","tags":["Arrangement","Indian Music","Jazz","Fusion","Quarantine Absurdity"],"title":"Fly Me To The Moon"},{"categories":["Music"],"contents":"This was a collaboration I did with Ronita Mookerji, a Bangalore based danseuse. In this piece she depicts how the Nava Rasas (nine emotions described in the ancient dance scriptures) play a role in the deep-seated complex myriad of emotions experienced by a woman in her urban existence through the pandemic.\nThe music for this piece was sung and arranged by me.\nCredits:\n Ronita Mookerji - Concept and Performer Gaby Davies - Video Edit Nithya Shikarpur - Vocals  The dance begins at the timestamp 19:38  \n","permalink":"https://snnithya.github.io/portfolio/nari/","tags":["Indian Music","Arrangement","Performance"],"title":"Nari (Woman) - a collaboration with Ronita Mookerji"},{"categories":["Music"],"contents":"This is a presentation of a kajri (semi-classical song). The lyrics are:\nKehenawa mano, O radha rani (Listen to my words, O radha rani)\nNishi andhiyari kari bijuri chamake, room-jhoom barasat paani (It is the night time and dark with lightning. It is raining heavily)\nHaath jod tori binati karata hoon, na maane mori bani (With my hands together, I am pleading with you. But you are not listening to me)\nThis song can be interpreted as being sung either by Radha\u0026rsquo;s beloved, a sakhi (friend) or her mother in an attempt to pacify her anger. I was taught this composition by my guru, Dr. Ruchira Kedar.\nI was accompanied by Sri Koushik Bhat on the Tabla.\n  ","permalink":"https://snnithya.github.io/portfolio/kajri/","tags":["Indian Music","Performance"],"title":"Kajri - Kehenawa Mano O Radha Rani"},{"categories":[""],"contents":"This is a portrait I made of my neighbour\u0026rsquo;s dog - Appu. Made with Adobe Illustrator on my laptop.\n","permalink":"https://snnithya.github.io/portfolio/appu/","tags":["Art"],"title":"Appu"},{"categories":["Music"],"contents":"This is an a cappella arrangement of the title track from the film Roja. I did the arrangement, vocals and recording for this piece.\n  ","permalink":"https://snnithya.github.io/portfolio/roja/","tags":["Arrangement","Indian Music","Quarantine Absurdity"],"title":"Roja - Cover"},{"categories":["Music"],"contents":"Having spent more time stuck at home than was good for me, I was struck by an inspiration to write an jingle for square wheels. Apart from their obvious defects in functionality, I think they have a pretty nice aesthetic going on :) So here is an absurd short song I wrote, arranged and recorded for square wheels. I made the animations on Blender.\n  Lyrics to the song Here are some wheels, but they are square wheels, they\u0026rsquo;re wheels in the shape of a square.\nThey don\u0026rsquo;t roll, just slide pretty nice, perfect for skiing on some ice.\nYou can\u0026rsquo;t get stuck in traffic. Absolutely no need to panic.\nDon\u0026rsquo;t move, just chill and sit along.\nNo more rolling baby, just got to slide like crazy\n\u0026lsquo;cause these aren\u0026rsquo;t round wheels after all.\n","permalink":"https://snnithya.github.io/portfolio/square_wheels/","tags":["Animation","Arrangement","Quarantine Absurdity","Art"],"title":"Square Wheels"},{"categories":["Other Research"],"contents":"From July 2019 to May 2020, I worked with Dr. Abhishek Tripathi as an intern at McAfee. This was my second project there and also resulted in a patent application. This project aimed to develop a malware detection model for partially downloaded binary files.\nInspiration This work was inspired by the research paper titled \u0026ldquo;Malware Images: Visualisation and Automatic Classification\u0026rdquo; by L. Nataraj et. al. The work developed an image based classifier to classify 25 different families of malware.\nOur Work Our work developed on this paper and extended it to detect malware from partially downloaded binary files as well.\nThis work has resulted in me being named inventor on a pending patent application (not yet published). This website will be updated when the application gets published.\n","permalink":"https://snnithya.github.io/portfolio/malware_binary/","tags":["Publication and Patent"],"title":"Detection of Malware in Partially Downloaded Binary Files"},{"categories":["Music"],"contents":"Here are a quick set of drawings I drew on my phone for the song \u0026lsquo;Amma nanu devarane\u0026rsquo; to put an Instagram story. This is a song about little Krishna. Caught with butter all over his mouth by his mother Yashoda, he stubbonrly swears that he didn\u0026rsquo;t steal any butter and that everyone else got together and put butter on his mouth to frame him. The visual nature of this song inspired me to draw it out a little.\n  ","permalink":"https://snnithya.github.io/portfolio/amma_naanu/","tags":["Art","Indian Music"],"title":"Amma Nanu Devarane - Devaranama"},{"categories":["Music"],"contents":"Here is our take on Waltz for Debby by Bill Evans. I had a lot of fun interpreting this very beautiful melody with Shikhar\u0026rsquo;s playing and adding a pinch of Indianness to the singing as well :)\nCredits\n Shikhar Rastogi - Keys Nithya Shikarpur - Vocals    ","permalink":"https://snnithya.github.io/portfolio/wfd/","tags":["Jazz","Indian Music","Fusion"],"title":"Waltz for Debby"},{"categories":["Music"],"contents":"This is a song by the poet Basavanna in praise of Lord Shiva. Performed at Sripada Kshetra, Bangalore, India for the occassion of Shivaratri. This song is set in raga Madhuvanti and Bhajan Theka tala.\nCredits:\n Koushik Bhat - Tabla (Percussion accompaniment) Nithya Shikarpur - Vocals    ","permalink":"https://snnithya.github.io/portfolio/ullavarushivalaya/","tags":["Indian Music","Performance"],"title":"Ullavarushivalaya - Song for shivaratri"},{"categories":["Other Research"],"contents":"From July 2019 to May 2020, I worked with Dr. Abhishek Tripathi as an intern at McAfee. This was my first project there and also contributed towards my undergraduate thesis. This project aimed to develop a malware detection system targeted for IoT devices.\nIoT devices Internet of Things (IoT) refers to smart devices that collect and share data over the internet. These devices have found a use case in almost every setting. Smart devices encompass a large spectrum of gadgets ranging from small wearable smart watches to bigger air conditioning systems. IoT devices have found their way into several fields including medicine, entertainment and governance.\nMotivation to develop malware detection for these devices:   IoT devices are becoming increasingly popular A report by Business Insider (dated 2017), states that there will be 30 billion IoT devices by the year 2020 [link]. This trend seems to only be going upwards since then.\n  Lack of security standards\nThis surge in IoT devices has developed too quickly for the security standards to be maintained. This has made these devices highly vulnerable to malware attacks.\n  Low compute resources on such devices\nMost of these devices were designed to collect and share data to a centralized server that process the data and sends back instructions. Hence due to the lesser amount of compute resources present on IoT devices, it is difficult to use traditional malware detection methods on the device itself.\n  TLS Features TLS (Transport Layer Security) is a cryptographic protocol that allows data being transmitted over the internet to be encrypted. While it is true that an the increased use of TLS over the internet, ensures greater security, it has also allowed malware to encrypt its traffic, thus making it hard to detect. Earlier methods of malware detection from network packets used features like the port numbers, IP addresses or patterns in the payload. These features cannot be used with encrypted traffic as they become unavailable with simple feature extraction.\nHence we rely on features that can be extracted from encrypted network traffic, i.e., TLS Features. Each TLS encrypted exchange of packets begins with a TLS handshake involving the exchange of a few unencrypted packets. We extract our features from this series of packets.\n Basic TLS handshake from where we extract features. [Image source link]\n  Models developed Logistic Regression A logistic regression model involves only the computation of a sigmoid function in order to give a prediction. This can easily be done on a router, with limited memory requirements. In order to ensure that the predictions can be made within a few milliseconds (to prevent the malware from getting passed on to the device from the router), we retained only the important features determined during our model training. Only these limited number of features were used for the model prediction, thus reducing the prediction time drastically.\nDeep Learning (Autoencoders) Since, in the real world, malware packets are usually an anomaly among mostly benign packets, we decided to experiment with autoencoders due to their good performance with anomaly detection.\nAlthough in actuality, malware data is much less seen than benign data, this was not the case with the dataset we had access to. Due to privacy issues, it is surprisingly more difficult to collect benign data. Hence we developed 2 autoencoders, trained on malware and benign data separately. A combination of the predictions from both models were used to determine whether a packet was malicious or not.\nThis project contributed towards my undergraduate thesis, available at this link.\n","permalink":"https://snnithya.github.io/portfolio/tls/","tags":["Publication and Patent"],"title":"Malware Detection for IoT devices"},{"categories":["Other Research"],"contents":"During a summer internship at Centre for infrastructure, Sustainable Transportation and Urban Planning (CiSTUP), Indian Institute of Sciences (IISc), Bangalore from May 2019 - July 2019, I worked on the visualisation and analysis of a phenomenon called bus bunching using data collected from BMTC buses in Bangalore. I was fortunate to work with Dr. Tarun Rambha who helped me navigate through domain knowledge that I wasn\u0026rsquo;t very familiar at the time.\nBus Bunching In public transport, bus bunching refers to the situation when 2 or more buses of the same route having evenly spaced schedules end up at the same place at the same time. For this, 1 or more buses have to violate their schedule. Causes for this could include traffic congestion, extra time taken by passengers to board or deborad the bus or temporary breakdowns of buses while on route. Effects of bus bunching can get magnified very easily. For instance, when two buses get bunched, the first bus usually gets overcrowded and the second goes near-empty. This becomes a vicious cycle that cannot be broken. This ultimately leads to inconsistent waiting times, not allowing passengers to fully rely on the bus system for their transport.\n Depiction of the effects of bus bunching. Taken from this link.\n  Data BMTC Bangalore Metropolitan Transport Corporation (BMTC) is a government agency that controls public transport bus service in Bengaluru.\nTypes of Data Collected We had access to ticketing and GPS data for a year from BMTC buses of multiple routes. Due to time constraints, I focused only on the data from January 1st, 2018. In addition, I chose only one route, with the most trips on this day - route #248 (Krishna Rajendra Market to Jalahalli Cross).\nVisualisation  Picture of visualisation of data\n  I have explained the meaning of all the components represented on the visualisation.\nMarkers Each marker represents a bus and is numbered in serial order. Buses bunched are green in colour and the other ones are red in colour.\nRoute Variables  Fare - Total fare collected by all buses on the route at a given point of time. Passengers - Total number of passangers that have boarded any bus on the route at a given point of time. Bunches - Number of bunching instances. Here, bunching instance is defined as an instant when 2 buses are travelling in the same direction but are within a certain bunching radius (in this case, 100m).  Route variables\n    Time Factor Time factor is defined as the number of seconds from the live data displayed per second in the simulation. This value can range from 10 to 1000.  Time factor\n  Bus Data It has the total fare collected and total number of passengers that have boarded the bus. The colour of the row changes based on whether it is bunched or not.  Bus data\n  Space Time Plot This a graph with the time passed on the X-axis and the distance travelled by the bus on the Y-axis (Distance is positive for the UP direction and negative for the DOWN direction). Each line plotted represents a di\u001bferent bus on the route. When two lines are parallel, it can be interpreted that they are consistently travelling with an even space between each other. However, when the lines intersect, we can see that they have bunched.  Space Time Plot\n  I used MySQL, Javascript (ChartJS), HTML and Python (pandas and modin) for this project. All the code associated along with a more detailed report can be found here\n","permalink":"https://snnithya.github.io/portfolio/bus_bunching/","tags":null,"title":"Bus Bunching Analysis and Visualisation"},{"categories":["Music"],"contents":"This is a cover of the song \u0026ldquo;Mist of Capricorn\u0026rdquo; originally performed by the band Agam. This is in the style of Carnatic fusion based on the song written by Saint Thyagaraja in raga Nalinakanti.\nCredits\n Vocals : Nithya, Shruti, Gargi, Simran Keyboards : Ritvik, Aravindh Electric Guitar : Bharat, Abhik Bass Guitar : Sagar Drums : Abhyuday Octapad : Vrajesh    ","permalink":"https://snnithya.github.io/portfolio/moc/","tags":["Indian Music","Performance","Fusion"],"title":"Mist Of Capricorn (Cover)"},{"categories":["Dance"],"contents":"I am a student of Bharathanatyam (Indian classical dance form) under the guidance of Guru P. Praveen Kumar. Here are small excerpts from a Varna in raga Todi, set to Adi tala. In this peice, the nayika (heroine) is expressing her love for her nayaka (hero), lord Shiva. This item was choreographed by my Guru.\nThis is from a performance conducted as a part of the Every Friday Cultural Evening Program (EFCEP) by Indian Council of Cultural Relations (ICCR) in collaboration with the Department of Kannada and Culture, Karnataka on 21 July, 2017.\nCredits\n Nattuvangam \u0026amp; Choreography - Guru Sri P. Praveen Kumar Vocals - Vid. D. S. Srivatsa Mridangam - Vid. Lingaraj S. Violin - Vid. Mysore R. Dayakar Dance - Nithya Shikarpur    ","permalink":"https://snnithya.github.io/portfolio/dance_rupamu/","tags":["Performance"],"title":"Bharathanatyam Dance Performance"}]